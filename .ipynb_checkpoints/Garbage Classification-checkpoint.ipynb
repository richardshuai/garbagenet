{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F  # useful stateless functions\n",
    "\n",
    "\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 384, 512)\n"
     ]
    }
   ],
   "source": [
    "valid_split = 0.2\n",
    "test_split = 0.2\n",
    "\n",
    "initial_transforms = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root='garbage_dataset/garbage_images', transform=initial_transforms)\n",
    "valid_size = int(0.2 * len(dataset))\n",
    "test_size = int(0.2 * len(dataset))\n",
    "train_size = len(dataset) - valid_size - test_size\n",
    "\n",
    "train_set, valid_set, test_set = random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "print(np.array(dataset[0][0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_r = np.dstack([np.array(train_set[i][0])[0, :, :] for i in range(len(train_set))])  \n",
    "data_g = np.dstack([np.array(train_set[i][0])[1, :, :] for i in range(len(train_set))])    \n",
    "data_b = np.dstack([np.array(train_set[i][0])[2, :, :] for i in range(len(train_set))])    \n",
    "\n",
    "means = (np.mean(data_r), np.mean(data_g), np.mean(data_b))\n",
    "stds = (np.std(data_r), np.std(data_g), np.std(data_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=means, std=stds)\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root='garbage_dataset/garbage_images', transform=data_transform)\n",
    "valid_size = int(0.2 * len(dataset))\n",
    "test_size = int(0.2 * len(dataset))\n",
    "train_size = len(dataset) - valid_size - test_size\n",
    "\n",
    "train_set, valid_set, test_set = random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=300, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=300, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=300, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part34(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(train_loader):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                check_accuracy_part34(valid_loader, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_part34(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(train_set[i][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.3077\n",
      "Got 117 / 505 correct (23.17)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "channel_1 = 32\n",
    "channel_2 = 16\n",
    "learning_rate = 1e-2\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Rewrite the 2-layer ConvNet with bias from Part III with the           #\n",
    "# Sequential API.                                                              #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, channel_1, kernel_size=5, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(channel_1, channel_2, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    Flatten(),\n",
    "    nn.Linear(32*32*channel_2, 10)\n",
    ")\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "train_part34(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
